# Status Update: Pushing Towards Beta (February 18th, 2026)

## Goals from Last Week:
- Complete listing ingestion and scraping module (Tanner)
- Implement structured text + image extraction (Daniel)
- Begin condition tagging service (CLIP integration) (Daniel)
- Develop comparable listings retrieval pipeline (Nathan)
- Connect real backend data to dashboard (replace mock data) (Lamont)
- Expand test coverage for scraping + database layer (David)
- Prepare demo slides for milestone presentation (Christy)

## What got done:
From last week, we completed a majority of the UI, backend systems, and general team structures (roles, schedules, meetings, etc.). 

From the list above, we accomplished:
- Complete listing ingestion and scraping module
    - Due to time constraints for the Beta release, we decided not to build our own scraping service and instead went with a third-party API hosted through RapidAPI. Through interfaces and custom type output, we can switch to a custom-made scraper later on with relative simplicity.
- Implement structured text + image extraction (Daniel)
    - We found that some Facebook Marketplace CDN image URLs could not be downloaded reliably (threw `invalid_image_url`), so image-based analysis would fail intermittently. To solve this, we fetched the image on our side and converted it into base64 data. Since this step was not 100% reliable, we had a fallback to use text-only. Text extraction was trivial in comparison.
- Begin condition tagging service (CLIP integration) (Daniel)
    - We utilized OpenAI GPT Vision 4o to create inferences based on the images and text within the listing. We were able to craft a prompt that reliably returns a JSON response with the proper fields for our components. We identified we could improve the results through refinement and additional examples in the context window, assigning it for the Gamma release.
- Develop comparable listings retrieval pipeline 
    - To reduce API calls, we used two tables: `condition_cache` and `listing_cache`. Before sending an API call, the backend queries the DB to see if the listing has been scraped before and if its condition has been calculated within the last 48 hours.
- Connect real backend data to dashboard (replace mock data)
    - This was done with the implementation of the tools above. Since we are waiting for responses from both OpenAI and RapidAPI, we implemented a loading skeleton to visually cue the user that the app is running but still loading.
- Expand test coverage for scraping + database layer
    - Additional testing files were made to be compliant with code coverage and our CI pipeline.
- Prepare demo slides for milestone presentation (Christy)
    - Collaborated together to consolidate our individual challenges and successes.
## Progress and Issues:
Mainly, our challenge was keeping things organized and flowing as the team faced holdups. Nevertheless, we accomplished our goals.
### Improvements:
We all became more comfortable and familiar with the tools being used. Exchanging work and ideas based on the tools in the software felt more like second nature. Additionally, we utilized the Scrum features in GitHub to have proper project coordination, such as branching, issues, and implementation backlog.
### New Learning: 
Since we all became comfortable with the tech stack, learning mainly came with quirks with interactions in the software:
- Some FB Marketplace listings don't contain images (e.g., video only). Our model would not work since it relies on images. The solution was to use a text-only fallback.
- There is another way to generate a listing link. The workflow we developed was based on links copied directly from the browser URL bar, but the alternate "share" menu link is unsupported.
- To prevent product hang-up, we implemented a timeout limit. We adjusted this as needed depending on how long it took to get a response from both APIs.
### Emerging Issues:
All issues we faced were fixed within ~2 hours. The only persistent issue we had was with `.env` variables...

Because a lot of changes were made to the backend, secrets and environment variables changed, which was not reflected in the GitHub Action secrets or the Vercel deployment secrets. It became challenging to securely share these keys. Our proposed solution is to use the EdStem chat, since it's also shared with TAs for testing.

## High Level Goals:
Our high level goals evolved with our development. As we push for Gamma and eventually final release, we noted these on our radar.
- Create *custom* scraping algorithm, with API fallback.
- Engineer a better prompt and fill context window with more listing information
- Implement similar listings and market trends (p1)
- Implement scam detection and risk factors (p2)

## Goals for Next Week: 
- Create User and Developer documentation (all)
- Migrate model confidence to use heuristics and have improved weights for various factors (Tanner)
- Implement similar listings and market price analysis (Daniel)
- Migrate default tailwind declarations to share constant styles (David)
- Custom scraping service (Tanner)
- Record demo video & product trailer (Nathan & Tanner)
- Expand testing to new features (Lamont & Christy)

## Questions for Product Leader:
If you had to change one side of the quarter, which side would it be? What would you change it to? 
